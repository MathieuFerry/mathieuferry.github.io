clean_text_vector <- clean_text(text_vector)
# Compter les occurrences
attitude_counts <- lapply(attitudes_dict, function(keywords) {
pattern <- paste(keywords, collapse = "|")
sum(str_count(clean_text_vector, pattern))
})
return(attitude_counts)
}
# Calculer les occurrences
attitude_results <- count_attitudes(text_data, attitudes)
# Préparer les données pour visualisation
attitude_df <- data.frame(
attitude = names(unlist(attitude_results)),
count = unlist(attitude_results)
) %>%
mutate(
groupe = case_when(
str_detect(attitude, "prof_") ~ "Professeurs",
str_detect(attitude, "eleve_") ~ "Élèves"
),
valence = case_when(
str_detect(attitude, "positif") ~ "Positif",
str_detect(attitude, "negatif") ~ "Négatif"
)
)
# Visualisation
ggplot(attitude_df, aes(x = attitude, y = count, fill = valence)) +
geom_bar(stat = "identity") +
facet_wrap(~groupe, scales = "free_x") +
labs(
title = "Attitudes des Professeurs et Élèves",
x = "Types d'attitudes",
y = "Nombre d'occurrences"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
attitude_results
text_data <- data$`Autres remarques` %>% na.omit() # Supprimer les valeurs manquantes
text_data <- obs$`Autres remarques` %>% na.omit() # Supprimer les valeurs manquantes
# Fonction de nettoyage du texte
clean_text <- function(text) {
text %>%
tolower() %>%
str_replace_all("[[:punct:]]", " ") %>%
str_squish()
}
# Dictionnaires d'attitudes
attitudes <- list(
# Attitudes positives des professeurs
prof_positif = c("receptif", "accueillant", "interesse", "comprehensif", "sympa", "bienveillant"),
# Attitudes négatives des professeurs
prof_negatif = c("agace", "presse", "refuse", "mecontent", "indifferent", "hostile"),
# Attitudes positives des étudiants
eleve_positif = c("interesse", "attentif", "enthousiaste", "curieux", "implique", "serieux"),
# Attitudes négatives des étudiants
eleve_negatif = c("ennuye", "indifferent", "bavard", "irrespectueux", "fatigue", "agace", "indispose")
)
# Fonction pour compter les occurrences d'attitudes
count_attitudes <- function(text_vector, attitudes_dict) {
# Nettoyer le texte
clean_text_vector <- clean_text(text_vector)
# Compter les occurrences
attitude_counts <- lapply(attitudes_dict, function(keywords) {
pattern <- paste(keywords, collapse = "|")
sum(str_count(clean_text_vector, pattern))
})
return(attitude_counts)
}
# Calculer les occurrences
attitude_results <- count_attitudes(text_data, attitudes)
# Préparer les données pour visualisation
attitude_df <- data.frame(
attitude = names(unlist(attitude_results)),
count = unlist(attitude_results)
) %>%
mutate(
groupe = case_when(
str_detect(attitude, "prof_") ~ "Professeurs",
str_detect(attitude, "eleve_") ~ "Élèves"
),
valence = case_when(
str_detect(attitude, "positif") ~ "Positif",
str_detect(attitude, "negatif") ~ "Négatif"
)
)
# Visualisation
ggplot(attitude_df, aes(x = attitude, y = count, fill = valence)) +
geom_bar(stat = "identity") +
facet_wrap(~groupe, scales = "free_x") +
labs(
title = "Attitudes des Professeurs et Élèves",
x = "Types d'attitudes",
y = "Nombre d'occurrences"
) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Analyse qualitative
print("Analyse des attitudes :")
for (groupe in c("prof", "eleve")) {
positif <- sum(attitude_results[[paste0(groupe, "_positif")]])
negatif <- sum(attitude_results[[paste0(groupe, "_negatif")]])
total <- positif + negatif
cat(
"Groupe :", ifelse(groupe == "prof", "Professeurs", "Élèves"), "\n",
"Total occurrences :", total, "\n",
"Attitudes positives :", positif,
"(", round(positif/total*100, 2), "%)\n",
"Attitudes négatives :", negatif,
"(", round(negatif/total*100, 2), "%)\n\n"
)
}
# Extraire quelques phrases significatives
find_significant_lines <- function(text_data, keywords) {
significant_lines <- lapply(keywords, function(keyword) {
grep(keyword, text_data, ignore.case = TRUE, value = TRUE)
})
unlist(significant_lines[sapply(significant_lines, length) > 0])
}
# Exemples de lignes significatives
cat("Exemples de descriptions d'attitudes :\n")
sig_lines <- find_significant_lines(text_data,
c("réceptif", "intéressé", "agacé", "refuse", "impliqué", "indifférent"))
print(head(sig_lines, 10))
library(tidytext)
# Charger un lexique de sentiment
sentiments <- get_sentiments("bing")
# Nettoyer les remarques et les transformer en mots
words <- tibble(text = cleaned_text) %>%
unnest_tokens(word, text)
# Associer chaque mot à un sentiment et calculer la polarité
sentiment_analysis <- words %>%
inner_join(sentiments, by = "word") %>%
count(sentiment)
# Visualisation
ggplot(sentiment_analysis, aes(x = sentiment, y = n, fill = sentiment)) +
geom_bar(stat = "identity") +
labs(title = "Analyse de sentiment des remarques", x = "Sentiment", y = "Fréquence") +
theme_minimal()
library(topicmodels)
# Préparation des données textuelles pour LDA
dtm <- words %>%
count(word) %>%
cast_dtm(document = row_number(), term = word, value = n)
words
words %>%
count(word)
words %>%
count(word) %>%
cast_dtm(document = row_number(), term = word, value = n)
# Préparation des données textuelles pour LDA
dtm <- words %>%
count(word) %>%
cast_dtm(document = row_number(), term = word, value = n)
?row_number()
# Préparation des données textuelles pour LDA
dtm <- words %>%
count(word) %>%
cast_dtm(document = row_number(), term = word, value = n)
# Préparation des données textuelles pour LDA
text_data_with_id <- tibble(id = row_number(), text = cleaned_text)
row_number()
# Préparation des données textuelles pour LDA
text_data_with_id <- tibble(
id = seq_along(cleaned_text),  # Crée un index unique pour chaque document
text = cleaned_text            # Texte nettoyé
)
# Tokenisation : diviser les remarques en mots
words <- text_data_with_id %>%
unnest_tokens(word, text)
# Créer la matrice document-terme
dtm <- words %>%
count(id, word) %>%  # Compter les occurrences de chaque mot par document
cast_dtm(document = id, term = word, value = n)
# Appliquer le modèle LDA
lda_model <- LDA(dtm, k = 3, control = list(seed = 123))
# Afficher les termes principaux pour chaque sujet
terms(lda_model, 5)
lda_model
dtm
words
tokens_remove(c(stopwords("fr")) %>%
p)
# Créer la matrice document-terme
dtm <- words %>%
tokens_remove(c(stopwords("fr"))) %>%
count(id, word) %>%  # Compter les occurrences de chaque mot par document
cast_dtm(document = id, term = word, value = n)
words
stopwords("fr")
words
# Créer la matrice document-terme
dtm <- words %>% filter(word!=stopwords("fr")) %>%
count(id, word) %>%  # Compter les occurrences de chaque mot par document
cast_dtm(document = id, term = word, value = n)
words %>% filter(word!=stopwords("fr"))
words
# Créer la matrice document-terme
dtm <- words %>% filter(!(word %in% stopwords("fr")) )%>%
count(id, word) %>%  # Compter les occurrences de chaque mot par document
cast_dtm(document = id, term = word, value = n)
# Appliquer le modèle LDA
lda_model <- LDA(dtm, k = 3, control = list(seed = 123))
# Afficher les termes principaux pour chaque sujet
terms(lda_model, 5)
# Afficher les termes principaux pour chaque sujet
terms(lda_model, 10)
# Appliquer le modèle LDA
lda_model <- LDA(dtm, k = 4, control = list(seed = 123))
# Afficher les termes principaux pour chaque sujet
terms(lda_model, 10)
# Afficher les termes principaux pour chaque sujet
topics <- tidy(lda_model, matrix = "beta")
# Récupérer les 5 termes les plus probables par sujet
top_terms <- topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
ggplot(top_terms, aes(x = reorder_within(term, beta, topic), y = beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, scales = "free") +
coord_flip() +
scale_x_reordered() +
labs(title = "Termes principaux par sujet",
x = "Termes",
y = "Probabilité beta")
# Récupérer les 5 termes les plus probables par sujet
top_terms <- topics %>%
group_by(topic) %>%
slice_max(beta, n = 15) %>%
ungroup() %>%
arrange(topic, -beta)
ggplot(top_terms, aes(x = reorder_within(term, beta, topic), y = beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, scales = "free") +
coord_flip() +
scale_x_reordered() +
labs(title = "Termes principaux par sujet",
x = "Termes",
y = "Probabilité beta")
# Appliquer le modèle LDA
lda_model <- LDA(dtm, k = 5, control = list(seed = 123))
# Afficher les termes principaux pour chaque sujet
terms(lda_model, 10)
# Afficher les termes principaux pour chaque sujet
topics <- tidy(lda_model, matrix = "beta")
# Récupérer les 5 termes les plus probables par sujet
top_terms <- topics %>%
group_by(topic) %>%
slice_max(beta, n = 15) %>%
ungroup() %>%
arrange(topic, -beta)
ggplot(top_terms, aes(x = reorder_within(term, beta, topic), y = beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, scales = "free") +
coord_flip() +
scale_x_reordered() +
labs(title = "Termes principaux par sujet",
x = "Termes",
y = "Probabilité beta")
# Appliquer le modèle LDA
lda_model <- LDA(dtm, k = 6, control = list(seed = 123))
# Afficher les termes principaux pour chaque sujet
terms(lda_model, 10)
# Afficher les termes principaux pour chaque sujet
topics <- tidy(lda_model, matrix = "beta")
# Récupérer les 5 termes les plus probables par sujet
top_terms <- topics %>%
group_by(topic) %>%
slice_max(beta, n = 15) %>%
ungroup() %>%
arrange(topic, -beta)
ggplot(top_terms, aes(x = reorder_within(term, beta, topic), y = beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~topic, scales = "free") +
coord_flip() +
scale_x_reordered() +
labs(title = "Termes principaux par sujet",
x = "Termes",
y = "Probabilité beta")
library(tidytext)
library(dplyr)
library(stringr)
library(tm)
text_data <- obs$`Autres remarques` %>% na.omit() # Supprimer les valeurs manquantes
setwd("/Users/mathieuferry/Documents/Recherche/Shiny apps/Basic textual analysis")
# Nettoyer les remarques et les transformer en mots
words <- tibble(text = cleaned_text) %>%
unnest_tokens(word, text)
stopwords<-read.csv2("french_stopwords.csv")
# Nettoyer les remarques et les transformer en mots
words <- tibble(text = cleaned_text) %>%
unnest_tokens(word, text)
# Créer la matrice document-terme
dtm <- words %>% filter(!(word %in% stopwords("fr")) )%>%
count(id, word) %>%  # Compter les occurrences de chaque mot par document
cast_dtm(document = id, term = word, value = n)
setwd("/Users/mathieuferry/Documents/Recherche/Shiny apps/Basic textual analysis")
stopwords<-read.csv2("french_stopwords.csv")
text_data <- obs$`Autres remarques` %>% na.omit() # Supprimer les valeurs manquantes
library(tidytext)
setwd("/Users/mathieuferry/Documents/Recherche/Shiny apps/Basic textual analysis")
stopwords<-read.csv2("french_stopwords.csv")
corpus <- corpus(text_data)
tok <- tokens(corpus, remove_punct = TRUE,
remove_symbols=TRUE,
remove_numbers =TRUE)
tok <- tokens_remove(tok,stopwords$token)
dtm <- dfm(tok, tolower = TRUE)
dtm <- dfm_trim(dtm, min_docfreq = 5)
textplot_wordcloud(dtm, random_order = F, rotation = 0.25,min_size =1,max_words = 100,
color = RColorBrewer::brewer.pal(8, "Dark2"))
dtm <- dfm_trim(dtm, min_docfreq = 2)
textplot_wordcloud(dtm, random_order = F, rotation = 0.25,min_size =1,max_words = 100,
color = RColorBrewer::brewer.pal(8, "Dark2"))
dtm <- dfm_trim(dtm, min_docfreq = 1)
textplot_wordcloud(dtm, random_order = F, rotation = 0.25,min_size =1,max_words = 100,
color = RColorBrewer::brewer.pal(8, "Dark2"))
dtm
library(FactoMineR)
CA(dtm)
tmod_ca <- textmodel_ca(dtm)
require(quanteda.textmodels)
require(quanteda.textmodels)
tmod_ca <- textmodel_ca(dtm)
dtm <- dfm_trim(dtm, min_docfreq = 2)
tmod_ca <- textmodel_ca(dtm)
dtm <- dfm_trim(dtm, min_docfreq = 2)
dtm <- dfm_trim(dtm, min_docfreq = 3)
tmod_ca <- textmodel_ca(dtm)
dtm <- dfm(tok, tolower = TRUE)
dtm <- dfm_trim(dtm, min_docfreq = 4)
tmod_ca <- textmodel_ca(dtm)
dtm
dtmm<-as.matrix.dtm(dtm)
dtmm<-as.matrix(dtm)
View(dtm)
View(dtmm)
tmod_ca <- textmodel_ca(dtm)
library(FactoMineR)
CA(dtmm)
PCA(dtmm)
setwd("/Users/mathieuferry/Documents/Recherche/Shiny apps/Basic textual analysis")
stopwords<-read.csv2("french_stopwords.csv")
corpus <- corpus(text_data)
tok <- tokens(corpus, remove_punct = TRUE,
remove_symbols=TRUE,
remove_numbers =TRUE)
tok <- tokens_remove(tok,stopwords$token)
dtm <- dfm(tok, tolower = TRUE)
dtm <- dfm_trim(dtm, min_docfreq = 2)
textplot_wordcloud(dtm, random_order = F, rotation = 0.25,min_size =1,max_words = 100,
color = RColorBrewer::brewer.pal(8, "Dark2"))
dtmm<-as.matrix(dtm)
library(FactoMineR)
PCA(dtmm)
CA(dtmm)
PCA(dtmm)
pca<-PCA(dtmm)
library(explor)
explor(pca)
library(ggthemes)
setwd("/Users/mathieuferry/Documents/Enseignements/Année 2024-2025/L2 Questionnaire/TD/Séance 9 Passation/ObsPassation")
obs<-read_xlsx("Observations.xlsx")
View(obs)
setwd("/Users/mathieuferry/Documents/Enseignements/Année 2024-2025/L3 ADD/Cours/Séance 8/TD")
setwd("/Users/mathieuferry/Documents/Enseignements/Année 2024-2025/L3 ADD/Cours/Séance 8/TD")
bdd<-read.csv2("TD8.csv")
bdd<-read.csv("TD8.csv")
library(questionr)
freq(enq$csp)
enq<-read.csv("TD8.csv")
library(questionr)
freq(enq$csp)
library(dplyr)
library(gtsummary)
names(enq)
var<-names(enq)
enq %>% tbl_summary(include=var)
tab1<-table(enq$csp,enq$assurvi)
lprop(tab1)
chisq.test(tab1)
cramer.v(tab1)
# Les réponses aux questions de cours ont été données au cours du TD.
# Les 4 premières questions de l'exercice ont été également répondues en cours.
options(warn=-1)
tab1<-table(enq$csp,enq$assurvi)
lprop(tab1)
chisq.test(tab1)
cramer.v(tab1)
tab1<-table(enq$csp,enq$assurvi)
tab1
lprop(tab1)
chisq.test(tab1)
cramer.v(tab1)
#Les indépendants (artisans et agriculteurs) ainsi que les intermédiaires, ouvriers, et retraités sont
#Davantage susceptibles que les autres d'avoir une assurance vie.
chisq.test(tab1)
tab1<-table(enq$sexe,enq$soldevu)
tab1
lprop(tab1)
chisq.test(tab1)
cramer.v(tab1)
tab1<-table(enq$age,enq$eparliv)
tab1
lprop(tab1)
chisq.test(tab1)
cramer.v(tab1)
tab1<-table(enq$sexe,enq$interdit)
tab1
lprop(tab1)
chisq.test(tab1)
cramer.v(tab1)
library(FactoMineR)
# Question 9 --------------------------------------------------------------
names(enq)
# Question 9 --------------------------------------------------------------
names(enq)
library(FactoMineR)
acm<-MCA(enq,quali.sup = c(1,4,5))
contrib1<-rownames(res.acm2$var$contrib)[res.acm2$var$contrib[,1]>100/nrow(res.acm2$var$contrib)]
sigsup1<-rownames(res.acm2$quali.sup$v.test)[abs(res.acm2$quali.sup$v.test[,1])>1.96]
fviz_mca_var(res.acm2,axes=c(1,2),
select.var=list(name = c(sigsup1,contrib1)))
contrib1<-rownames(acm$var$contrib)[acm$var$contrib[,1]>100/nrow(acm$var$contrib)]
sigsup1<-rownames(acm$quali.sup$v.test)[abs(acm$quali.sup$v.test[,1])>1.96]
fviz_mca_var(acm,axes=c(1,2),
select.var=list(name = c(sigsup1,contrib1)))
library(explor)
library(factoextra)
explor(acm)
100/57
explor(acm)
#Voir le corrigé en ligne.
plot(acm)
?plot
?plot.acm
?MCA
#Voir le corrigé en ligne.
plot(acm,choix="var")
library(tidyverse)
options(java.parameters = "-Xmx8000m")
library(rJava)
library(xlsx)
options(warn=-1)
setwd("/Users/mathieuferry/Documents/Enseignements/Année 2024-2025/L2 Questionnaire/Base 2023-2024")
pof<-read.xlsx2("etubud_v2 - AO.xlsx",sheetIndex = 1)
table(pof$pcs_men_grp)
table(pof$nivet_mer)
pof<-pof %>% mutate(nivet_par=case_when(nivet_per=="Bac +5 ou plus" | nivet_mer=="Bac +5 ou plus"~"Bac +5 ou plus",
nivet_per=="Bac +3 à +4" | nivet_mer=="Bac +3 à +4"~"Bac +3 à +4",
nivet_per=="Bac +1 à +2" | nivet_mer=="Bac +1 à +2"~"Bac +1 à +2",
nivet_per=="Bac" | nivet_mer=="Bac"~"Bac",
nivet_per=="Aucun ou BEPC" | nivet_mer=="Aucun ou BEPC"~"Aucun ou BEPC",
nivet_per %in% c("","Je ne sais pas") | nivet_mer %in% c("","Je ne sais pas")~""
))
table(pof$nivet_par)
pof <- pof %>%
mutate_all(
funs(case_when(
. == "" | .=="Autre : précisez ..." | .=="0" | .=="Je ne sais pas" | .=="Autre(s) : précisez..." ~ "Na",
T~.)))
pof <- pof %>%
mutate(across(everything(), ~ as_factor(.)))
pofred<-pof %>% select(
qualitedepalim_promo	,
qualitedepalim_label,
qualitedepalim_disco,
qualitedepalim_reduc	,
qualitedepalim_perim	,
ressourcesperso_jobvac,
ressourcesperso_jobpar,
#volalim,
achatvet,
fraistransport,
fraudetransport,
departvacances,
avecquihabiter,
# diffbudgetrepetees_faim,
diffbudgetrepetees_varalim,
diffbudgetrepetees_trocher ,
natureaides_bours,
# diffbudgetrepetees_privmed,
univ,domaine_rec,niveau,genre_bis
)
library(FactoMineR)
library(GDAtools)
getindexcat(pofred)
indexmod<-getindexcat(pofred)[1:52]
modNA<-which(substr(indexmod, nchar(indexmod)-1, nchar(indexmod))=="Na")
names(pofred)
res.mca<-MCA(pofred,quali.sup=c(16:19),excl=modNA)
library(explor)
explor(res.mca)
fviz_eig(res.mca, addlabels = TRUE, ylim = c(0, 85))
fviz_eig(res.mca, addlabels = TRUE, ylim = c(0, 85))
library(factoextra)
fviz_eig(res.mca, addlabels = TRUE, ylim = c(0, 85))
fviz_eig(res.mca, addlabels = TRUE)
?fviz_eig
100/52
library(kableExtra)
eig<-res.mca$eig
kable(eig,digits=c(3,1,1,1,1)) %>% kable_styling()
fviz_mca_ind(res.mca,alpha.ind = "contrib",label="none")
contrib1<-rownames(res.mca$var$contrib)[res.mca$var$contrib[,1]>100/nrow(res.mca$var$contrib)]
fviz_mca_var(res.mca,axes=c(1,2),
select.var=list(name = contrib1))
sigsup1<-rownames(res.mca$quali.sup$v.test)[abs(res.mca$quali.sup$v.test[,1])>1.96]
fviz_mca_var(res.mca,axes=c(1,2),
select.var=list(name = c(sigsup1,contrib1)))
quarto render --profile english
